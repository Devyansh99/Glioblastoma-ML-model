{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved at D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# File paths\n",
    "clinical_file = r\"D:\\mlpr data\\Glioblastoma-ML-model\\UPENN-GBM_clinical_info_v2.1.csv\"\n",
    "radiomics_folder = r\"D:\\mlpr data\\radiomic_features_CaPTk\"\n",
    "\n",
    "# Load clinical data\n",
    "clinical_df = pd.read_csv(clinical_file)\n",
    "clinical_df.rename(columns={\"ID\": \"PatientID\"}, inplace=True)  # Standardizing ID column name\n",
    "\n",
    "# Load all radiomic CSVs and merge horizontally on PatientID\n",
    "radiomic_files = glob(os.path.join(radiomics_folder, \"*.csv\"))\n",
    "\n",
    "# Initialize empty dataframe for radiomics\n",
    "radiomics_df = pd.DataFrame()\n",
    "\n",
    "for file in radiomic_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.rename(columns={\"SubjectID\": \"PatientID\"}, inplace=True)  # Standardizing ID column name\n",
    "    \n",
    "    # Merge radiomics files horizontally\n",
    "    if radiomics_df.empty:\n",
    "        radiomics_df = df\n",
    "    else:\n",
    "        radiomics_df = pd.merge(radiomics_df, df, on=\"PatientID\", how=\"outer\")\n",
    "\n",
    "# Merge clinical data with radiomics data\n",
    "merged_df = pd.merge(clinical_df, radiomics_df, on=\"PatientID\", how=\"outer\")\n",
    "\n",
    "# Save final merged dataset\n",
    "output_file = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved at {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_26020\\2633508008.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (forcing errors='coerce' turns non-numeric values into NaN)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\"])  # Drop ID and target\n",
    "y = df[\"Survival_from_surgery_days_UPDATED\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoder for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 342.39\n",
      "R² Score: 0.0764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)  # Use all cores\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 342.39\n",
      "R² Score: 0.0764\n",
      "Root Mean Squared Error (RMSE): 475.31\n",
      "Explained Variance Score: 0.0794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Root Mean Squared Error\n",
    "evs = explained_variance_score(y_test, y_pred)  # Explained Variance Score\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Explained Variance Score: {evs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_26020\\1281962872.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.82      0.70        76\n",
      "           1       0.36      0.25      0.29        40\n",
      "           2       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.56       129\n",
      "   macro avg       0.32      0.36      0.33       129\n",
      "weighted avg       0.47      0.56      0.50       129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (handling non-numeric values)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# **Convert Survival Days into Categories (Example Binning)**\n",
    "# You can modify bins as per your requirement\n",
    "bins = [0, 450, 1100, np.inf]  # <100 days, 100-365 days, >365 days\n",
    "\n",
    "labels = [0, 1, 2]  # Class labels\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_34328\\3368345720.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.93      0.71        76\n",
      "           1       0.17      0.03      0.04        40\n",
      "           2       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.56       129\n",
      "   macro avg       0.25      0.32      0.25       129\n",
      "weighted avg       0.39      0.56      0.43       129\n",
      "\n",
      "Number of PCA components used: 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (handling non-numeric values)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# **Convert Survival Days into Categories (Example Binning)**\n",
    "bins = [0, 450, 1100, np.inf]  # Define bin edges\n",
    "labels = [0, 1, 2]  # Class labels\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.9)  # Retain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Print the number of PCA components\n",
    "print(f\"Number of PCA components used: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (671, 9516)\n",
      "Dropped 27 rows with missing target values\n",
      "\n",
      "Columns with mixed data types:\n",
      "- Time_since_baseline_preop: ['int', 'str']\n",
      "\n",
      "Class distribution:\n",
      "Survival_Category\n",
      "2    0.526398\n",
      "0    0.267081\n",
      "1    0.206522\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Ensure target column exists\n",
    "if \"Survival_from_surgery_days_UPDATED\" not in df.columns:\n",
    "    print(\"Error: Target column 'Survival_from_surgery_days_UPDATED' not found!\")\n",
    "    print(\"Available columns:\", df.columns)\n",
    "    exit()\n",
    "\n",
    "# Data cleaning - convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "initial_count = df.shape[0]\n",
    "df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"], inplace=True)\n",
    "print(f\"Dropped {initial_count - df.shape[0]} rows with missing target values\")\n",
    "\n",
    "# === DATA TYPE INSPECTION AND FIXING ===\n",
    "# First, let's check for mixed types in columns\n",
    "def check_mixed_types(dataframe):\n",
    "    mixed_cols = []\n",
    "    for col in dataframe.columns:\n",
    "        # Check if column has mixed types\n",
    "        types = dataframe[col].apply(type).unique()\n",
    "        if len(types) > 1:\n",
    "            mixed_cols.append((col, types))\n",
    "    return mixed_cols\n",
    "\n",
    "# Identify mixed type columns\n",
    "mixed_type_cols = check_mixed_types(df)\n",
    "print(\"\\nColumns with mixed data types:\")\n",
    "for col, types in mixed_type_cols:\n",
    "    print(f\"- {col}: {[t.__name__ for t in types]}\")\n",
    "\n",
    "# Convert all object columns to string type to prevent issues\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Make sure there are no numeric columns with string values\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_cols:\n",
    "    # Check if column has any string values\n",
    "    try:\n",
    "        pd.to_numeric(df[col])\n",
    "    except:\n",
    "        print(f\"Converting mixed column {col} to string\")\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "# Feature engineering\n",
    "# 1. Add log transformation for skewed survival days\n",
    "df[\"Log_Survival_Days\"] = np.log1p(df[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# 2. Binning target variable into 3 categories based on clinical thresholds\n",
    "# For GBM patients: poor (<6 months), intermediate (6-12 months), good (>12 months)\n",
    "survival_thresholds = [0, 180, 365, float('inf')]  # 6 months, 1 year\n",
    "labels = [0, 1, 2]  # Poor, Medium, Good survival\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=survival_thresholds, labels=labels)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "class_dist = df[\"Survival_Category\"].value_counts(normalize=True)\n",
    "print(class_dist)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\", \"Log_Survival_Days\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# === FEATURE SELECTION BY CORRELATION ===\n",
    "# For numerical features, drop highly correlated features (>0.95)\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64'])\n",
    "if not numerical_features.empty:\n",
    "    correlation = numerical_features.corr().abs()\n",
    "    upper_tri = correlation.where(np.triu(np.ones(correlation.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "    print(f\"\\nDropping {len(to_drop)} highly correlated numerical features\")\n",
    "    X = X.drop(columns=to_drop, errors='ignore')\n",
    "\n",
    "# Identify categorical and numerical columns again after clean-up\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nAfter cleaning and feature selection:\")\n",
    "print(f\"- Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"- Numerical features: {len(numerical_cols)}\")\n",
    "\n",
    "# Better preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Changed from KNNImputer for stability\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\nApplying preprocessing transformations...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "\n",
    "# Handle class imbalance with SMOTE if needed\n",
    "class_counts = np.bincount(y_train.astype(int))\n",
    "if max(class_counts) / min(class_counts) > 1.5:  # If imbalance ratio > 1.5\n",
    "    print(\"\\nApplying SMOTE for class balancing...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    print(\"Original training class distribution:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(\"Resampled training class distribution:\")\n",
    "    print(pd.Series(y_train_resampled).value_counts())\n",
    "else:\n",
    "    X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "# === MODEL SELECTION AND TRAINING ===\n",
    "# Try multiple classifiers and choose the best one\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "print(\"\\nTraining and evaluating multiple models...\")\n",
    "best_accuracy = 0\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Balanced Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model_name = name\n",
    "        best_model = model\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} (Balanced Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "# Fine-tune the best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "else:  # XGBoost\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "\n",
    "print(f\"\\nFine-tuning {best_model_name}...\")\n",
    "search = RandomizedSearchCV(\n",
    "    classifiers[best_model_name],\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Reduced for quicker results\n",
    "    cv=StratifiedKFold(3),  # Using 3-fold CV for speed\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train_resampled, y_train_resampled)\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Evaluate the final model\n",
    "y_pred = best_model.predict(X_test_processed)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Poor', 'Medium', 'Good'], \n",
    "            yticklabels=['Poor', 'Medium', 'Good'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    categorical_feature_names = []\n",
    "    \n",
    "    for name, transformer, cols in preprocessor.transformers_:\n",
    "        if name == 'cat':\n",
    "            # For categorical features, we need to get the one-hot encoded feature names\n",
    "            ohe = transformer.named_steps.get('onehot')\n",
    "            if ohe:\n",
    "                try:\n",
    "                    transformed_names = ohe.get_feature_names_out(cols)\n",
    "                    categorical_feature_names.extend(transformed_names)\n",
    "                except:\n",
    "                    # If can't get feature names, use placeholders\n",
    "                    categorical_feature_names.extend([f'cat_{i}' for i in range(ohe.n_features_in_)])\n",
    "        else:\n",
    "            # For numerical features, use the original column names\n",
    "            feature_names.extend(cols)\n",
    "    \n",
    "    # Combine all feature names\n",
    "    all_feature_names = categorical_feature_names + feature_names\n",
    "    \n",
    "    # Get feature importances from the model\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Match feature names with importances\n",
    "    # Note: This might not perfectly match if the preprocessing changed the number of features\n",
    "    if len(all_feature_names) == len(importances):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': all_feature_names,\n",
    "            'Importance': importances\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Display top features\n",
    "        print(\"\\nTop 15 important features:\")\n",
    "        print(feature_importance.head(15))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = feature_importance.head(15)\n",
    "        sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "        plt.title(f'Top 15 Features by Importance - {best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"feature_importance.png\")\n",
    "    else:\n",
    "        print(\"\\nCouldn't match feature names with importances. Lengths differ.\")\n",
    "        print(f\"Feature names: {len(all_feature_names)}, Importances: {len(importances)}\")\n",
    "\n",
    "# Save the model and results\n",
    "import joblib\n",
    "joblib.dump(preprocessor, 'glioblastoma_preprocessor.pkl')\n",
    "joblib.dump(best_model, 'glioblastoma_best_model.pkl')\n",
    "\n",
    "print(\"\\nModel training and evaluation complete. Files saved.\")\n",
    "print(\"To use this model for predictions:\")\n",
    "print(\"1. Load the saved preprocessor and model\")\n",
    "print(\"2. Preprocess new data with the preprocessor\")\n",
    "print(\"3. Use the model to make predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
